<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

    <meta http-equiv="content-type" content="text/html; charset=windows-1252">
  </head>
  <body>
    <h2>Evaluation</h2>
    <a name="evaluation"></a>
    <h3>Tasks</h3>
    This challenge is concerned with <b>regression</b> and <b>classification
      problems</b> (binary, multi-class, or multi-label) from data
    already formatted in fixed-length feature-vector representations.
    Each task is associated with a dataset coming from a real
    application. The domains of application are very diverse and are
    drawn from: biology and medicine, ecology, energy and sustainability
    management, image, text, audio, speech, video and other sensor data
    processing, internet social media management and advertising, market
    analysis and financial prediction. <br>
    All datasets present themselves in the form of data matrices with
    samples in lines and features (or variables) in columns. For
    instance, in a medical application, the samples may represent
    patient records and the features may represent results of laboratory
    analyses.
    The goal is to predict a target value, for instance the diagnosis
    "diseased" or "healthy" in the case of a medical diagnosis problem.
    <br>
    The identity of the datasets and the features is concealed (except
    in round 0) to avoid the use of domain knowledge and push the
    participants to design fully automated machine learning solutions.
    <br>
    In addition, the tasks are constrained by:
    <ul>
      <li> A Time Budget.
      </li>
      <li> A Scoring Metric.
      </li>
    </ul>
    Task, scoring metric and time budget are provided with the data, in
    a special "info" file.
    <h3>Time Budget</h3>
    <a name="time"></a>
    <p>The Codalab platform provides computational resources shared by
      all participants. To ensure the fairness of the evaluation, when a
      code submission is evaluated, its execution time is limited to a
      given Time Budget, which varies from dataset to dataset. The time
      budget is provided with each dataset in its "info" file.
      The organizers reserve the right to adjust the time budget by
      supplying the participants with new info files.
      <br>
      The participants who submit results (instead of code) are NOT
      constrained by the Time Budget, since they can run their code on
      their own platform. This may be advantageous for entries counting
      towards the Final phases (immediately following a Tweakathon). The
      participants wishing to also enter the AutoML phases, which
      require submitting code, can
      submit BOTH results and code (simultaneously). See the <a
        href="#learn_the_details-instructions" data-toggle="tab">Instructions</a>
      for details.
    </p>
    <h3>Scoring Metrics</h3>
    <a name="evaluation"></a>
    The scoring program computes a score by comparing submitted
    predictions with reference "target values". For each sample i,
    i=1:P, the target value is:
    <ul>
      <li> a continuous numeric coefficient y<small>i</small>, for
        regression problem;
      </li>
      <li> a vector of binary indicators [y<small>ik</small>] in {0, 1},
        for multi-class or multi-label classification problems (one per
        class k);
      </li>
      <li> a single binary indicator y<small>i</small> in {0, 1}, for
        binary classification problems.
      </li>
    </ul>
    The participants must turn in prediction values matching as closely
    as possible the target value, in the form of:
    <ul>
      <li> a continuous numeric coefficient q<small>i</small> for
        regression problem;
      </li>
      <li> a vector of numeric coefficients [q<small>ik</small>] in the
        range [0, 1] for multi-class or multi-label classification
        problems (one per class k);
      </li>
      <li> a single numeric coefficients q<small>i</small> in the range
        [0, 1] for binary classification problems.
      </li>
    </ul>
    The <a
      href="http://www.causality.inf.ethz.ch/AutoML/Starting_kit.zip">Starting
      Kit</a> contains the Python implementation of all scoring metrics
    used to evaluate the entries. <b>Each dataset has its own metric</b>
    (scoring criterion), specified in its "info" file. <b>All scores
      are re-normalized</b> such that the expected value of the score
    for a "trivial guess" based on class prior probabilities is 0 and
    the optimal score is 1. Multi-label problems are treated as multiple
    binary classification problems and are evaluated by the average of
    the scores of each binary classification sub-problem.<br>
    The scores are taken from the following list:<br>
    <ul>
      <li><b><a
            href="http://en.wikipedia.org/wiki/Coefficient_of_determination">R2</a></b>:
        R-square or "coefficient of determination" used for regression
        problems: <font color="#3333ff"><b>R2 = 1-MSE/VAR</b></font>,
        where MSE=&lt; <small></small>(y<small>i</small> - q<small>i</small>)<sup>2</sup>&gt;
        is the mean-square-error and VAR= &lt; (y<small>i </small>- m)<sup>2</sup>&gt;
        is the variance, with m=&lt; y<small>i </small>&gt;. </li>
      <li><b>ABS</b>: A coefficient similar to the R2 but based on mean
        absolute error (MAE) and mean absolute deviation (MAD): <font
          color="#3333ff"><b>ABS =&nbsp; </b><b>1-MAE/MAD</b></font>,
        with MAE=&lt; abs(y<small>i </small>- q<small>i</small>) &gt;
        and MAD=&lt; abs(y<small>i </small>- m) &gt;.
      </li>
      <li> <b>BAC</b>: Balanced accuracy, which is the <b><a
href="http://spokenlanguageprocessing.blogspot.fr/2011/12/evaluating-multi-class-classification.html">average
            of class-wise accuracy</a></b> for classification problems
        (or the <a
          href="http://en.wikipedia.org/wiki/Accuracy_and_precision">average
          of sensitivity (true positive rate) and specificity (true
          negative rate)</a> for the special case of binary
        classification). For binary classification problems, the
        class-wise accuracy is the fraction of correct class predictions
        when q<small>i</small> is thresholded at 0.5, for each class.
        The class-wise accuracy is averaged over all classes for
        multi-label problems. For multi-class classification problems,
        the predictions are binarized by selecting the class with
        maximum prediction value argmax<small>k</small> q<small>ik</small>
        before computing the class-wise accuracy. We normalize the BAC
        with the formula BAC := (BAC-R)/(1-R), where R is the expected
        value of BAC for random predictions (i.e. R=0.5 for binary
        classification and R=(1/C) for C-class classification problems).<br>
      </li>
      <li> <b>AUC</b>: <b><a
            href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">Area
            under the ROC curve</a></b>, used for ranking and for binary
        classification problems. The ROC curve is the curve of
        sensitivity vs. 1-specificity, when a threshold is varied on the
        predictions. The AUC is identical to the BAC for binary
        predictions. The AUC is calculated for each class separately
        before averaging over all classes. We normalize it with the
        formula: AUC := 2AUC-1, making it de-facto identical to the
        so-called Gini index.<br>
      </li>
      <li> <b><a href="http://en.wikipedia.org/wiki/F1_score">F1</a></b>
        score: The <b><a
            href="http://en.wikipedia.org/wiki/Harmonic_mean">harmonic
            mean</a></b><b> <font color="#3333ff">of precision and
            recall</font></b><font color="#3333ff">.</font>
        Precision=positive predictive
        value=true_positive/all_called_positive. Recall=sensitivity=true
        positive rate=true_positive/all_real_positive. Prediction
        thresholding and class averaging is handled similarly as in the
        case of the BAC. We also normalize F1 with F1 := (F1-R)/(1-R),
        where R is the expected value of F1 for random predictions (i.e.
        R=0.5 for binary classification and R=(1/C) for C-class
        classification problems).</li>
      <li> <b>PAC</b>: Probabilistic accuracy <font color="#3333ff"><b>PAC
            = exp(- CE) </b></font>based on the <a
          href="http://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a>
        or log loss, CE = - &lt;<small> </small>sum<small>k </small>log(q<small>ik</small>)
        &gt; for multi-class classification and CE = - &lt;y<small>i </small>log(q<small>i</small>)
        + (1-y<small>i</small>) log(1-q<small>i</small>)&gt; for binary
        classification and multi-label problems. Class averaging is
        performed after taking the exponential in the multi-label case.
        We normalize with PAC := (PAC-R)/(1-R), where R is the score
        obtained using q<small>i</small> =&lt; y<small>i </small>&gt;
        or q<small>ik</small>=&lt; y<small>ik </small>&gt; (i.e. using
        as predictions the fraction of positive class examples as an
        estimate of the prior probability).</li>
    </ul>
    We note that for R2, ABS, and PAC the normalization uses a "trivial
    guess" corresponding to the average target value q<small>i</small>
    =&lt; y<small>i </small>&gt; or q<small>ik</small>=&lt; y<small>ik
    </small>&gt;. In contrast, for BAC, AUC, and F1 the "trivial guess"
    is a random prediction of one of the classes with uniform
    probability.<br>
    In all formulas the brackets &lt; . &gt; designates the average over
    all P samples indexed by i: &lt; y<small>i </small> &gt; = (1/P)
    sum<small>i </small> (y<small>i</small>). Only R2 and ABS make
    sense for regression; we compute the other scores for completeness
    by replacing the target values by binary values after thresholding
    them in the mid-range.<br>
    <h3>Leaderboard score calculation</h3>
    Each round includes five datasets from different application
    domains, spanning various levels of difficulty. The participants (or
    their submitted programs) provide prediction results for the
    withheld target values (called "solution"), for all 5 datasets.
    <b>Independently of any intervention of the participants</b>, the
    original version of the scoring program
    supplied by the organizers is run on the server to compute the
    scores. For each dataset, the participants are
    ranked in decreasing order of performance for the prescribed scoring
    metric associated with the given task.
    The overall score is computed by averaging the ranks over all 5
    datasets and shown in the column &lt;rank&gt; on the leaderboard.
    <p>We ask the participants to test their systems regularly while
      training to produce intermediate prediction results, which will
      allow us to make learning curves (performance as a function of
      training time). Using such learning curves, we will adjust the
      "time budget" in subsequent rounds (eventually giving you more
      computational time!).
      But only the last point (corresponding to the file with the
      largest order number) is used for leaderboard calculations.
    </p>
    <p>The results of the LAST submission made are used to compute the
      leaderboard results (so you must re-submit an older entry that you
      prefer if you want it to count
      as your final entry). This is what is meant by “Leaderboard
      modifying disallowed”.
      In phases marked with a [+], the participants with the three
      smallest &lt;rank&gt; are eligible for prizes, if they meet the <a
        href="#learn_the_details-terms_and_conditions" data-toggle="tab">Terms
        and Conditions</a>.
    </p>
    <h3>Training, validation and test sets</h3>
    For each dataset, a labeled training set is provided for training
    and two unlabeled sets (validation set and test set) are provided
    for testing.
    <h3>Phases and rounds</h3>
    The challenge is run in multiple <a href="#phases"
      data-toggle="tab">Phases</a> grouped in rounds, alternating AutoML
    contests and Tweakathons. There are 6 six rounds: Round 0
    (Preparation round), followed by 5 rounds of progressive difficulty
    (Novice, Intermediate, Advanced, Expert, and Master).
    Except for round 0 (preparation) and round 5 (termination), all
    rounds include 3 phases, alternating Tweakathons and AutoML
    contests:
    <table cellpadding="2" cellspacing="2" border="1" width="100%">
      <tbody>
        <tr>
          <td valign="top"><b>Phase in round [n]</b><br>
          </td>
          <td valign="top"><b>Goal</b><br>
          </td>
          <td valign="top"><b>Duration</b><br>
          </td>
          <td valign="top"><b>Submissions</b></td>
          <td valign="top"><b>Data</b><br>
          </td>
          <td valign="top"><b>Leaderboard scores</b></td>
          <td valign="top"><b>Prizes</b><br>
          </td>
        </tr>
        <tr>
          <td valign="top">[+] AutoML[n]</td>
          <td valign="top">Blind test of <b>code </b><br>
          </td>
          <td valign="top">Short<br>
          </td>
          <td valign="top">NONE (code migrated)<br>
          </td>
          <td valign="top">New datasets, not downloadable<br>
          </td>
          <td valign="top">Test set results</td>
          <td valign="top">Yes<br>
          </td>
        </tr>
        <tr>
          <td valign="top">Tweakathon[n]</td>
          <td valign="top">Manual tweaking<br>
          </td>
          <td valign="top">1 month<br>
          </td>
          <td valign="top">Code and/or results</td>
          <td valign="top">Datasets downloadable</td>
          <td valign="top">Validation set results</td>
          <td valign="top">No<br>
          </td>
        </tr>
        <tr>
          <td valign="top">[+] Final[n]</td>
          <td valign="top">Results of Tweakathon revealed<br>
          </td>
          <td valign="top">Short<br>
          </td>
          <td valign="top">NONE (results migrated)<br>
          </td>
          <td valign="top">NA<br>
          </td>
          <td valign="top">Test set results</td>
          <td valign="top">Yes<br>
          </td>
        </tr>
      </tbody>
    </table>
    <br>
    The results of
    the last submission made are shown on the leaderboard. <b>Submissions
      are made in Tweakathon phases only.</b> The last
    submission of one phase migrates automatically to the next one.
    If code is submitted, this makes it possible to participate to
    subsequent phases without making new submissions.
    Prizes are attributed for phases marked with a [+] during which
    there is NO submission. The total prize pool is $30,000 (see <a
      href="#learn_the_details-rewards" data-toggle="tab">Rewards</a>
    and <a href="#learn_the_details-terms_and_conditions"
      data-toggle="tab">Terms and Conditions</a> for details).
    <h3>Code vs. result submission</h3>
    To participate in the AutoML[n] phase, code must be submitted in
    Tweakathon[n-1]. To participate in the Final[n], code or
    results must be submitted in Tweakathon[n]. If both code and
    (well-formatted) results are submitted, in&nbsp; Tweakathon[n] the
    results are used for scoring rather than re-running the code in
    Tweakathon[n] and Final[n]. The code is executed when results are
    unavailable or not well formatted. Hence there is no disadvantage to
    submitting both results and code. There is no obligation to submit
    the code, which has produced the results provided. Using mixed
    submissions of results and code, different methods can be used to
    enter the Tweakathon/Final phases and to enter the AutoML phases.<br>
    <h3>Datasets</h3>
    There are 5 datasets in each round spanning a range of difficulties:
    <ul>
      <li> <b>Different tasks:</b> regression, binary classification,
        multi-class classification, multi-label classification.
      </li>
      <li> <b>Class balance:</b> Balanced or unbalanced class
        proportions.
      </li>
      <li> <b>Sparsity:</b> Full matrices or sparse matrices.
      </li>
      <li> <b>Missing values:</b> Presence or absence of missing
        values.
      </li>
      <li> <b>Categorical variables:</b> Presence or absence of
        categorical variables.
      </li>
      <li> <b>Irrelevant variables:</b> Presence or absence of
        additional irrelevant variables (distractors).
      </li>
      <li> <b>Number Ptr of training examples:</b> Small or large
        number of training examples.
      </li>
      <li> <b>Number N of variables/features:</b> Small or large number
        of variables.
      </li>
      <li> <b>Aspect ratio Ptr/N of the training data matrix:</b>
        Ptr&gt;&gt;N, Ptr~=N or Ptr&lt;&lt;N.
      </li>
    </ul>
    We will progressively introduce difficulties from round to round
    (each round <b>cumulating all the difficulties of the previous ones
      plus new ones</b>):
    Some datasets may be recycled from previous challenges, but
    reformatted into new representations, except for the final MASTER
    round, which includes only completely new data.
    <ol>
      <li> <b> NOVICE: Binary classification problems</b> only; no
        missing data; no categorical variables; moderate number of
        features (&lt;2000); balanced classes; BUT <b>sparse and full
          matrices; presence of irrelevant variables; various Ptr/N</b>.
      </li>
      <li> <b> INTERMEDIATE: Multi-class and binary classification
          problems </b> + additional difficulties including: <b>unbalanced
          classes; small and large number of classes (several hundred);
          some missing values; some categorical variables; up to 5000
          features.</b>
      </li>
      <li> <b> ADVANCED: All types of classification problems,
          including multi-label </b> + additional difficulties
        including: <b> up to 300,000 features.</b>
      </li>
      <li> <b> EXPERT: Classification and regression problems</b>, all
        difficulties.
      </li>
      <li> <b> MASTER: Classification and regression problems</b>, all
        difficulties, completely new datasets.
      </li>
    </ol>
    <br>
    <br>
    <p><a href="http://chalearn.org"><img
          src="http://www.causality.inf.ethz.ch/styles/chalearn.png"
          alt="CHALEARN" align="left" border="0" width="100"></a></p>
    <p>This challenge is brought to you by <a
        href="http://chalearn.org">ChaLearn</a>. <a
        href="mailto:events@chalearn.org">Contact the organizers</a>.</p>
  </body>
</html>
